{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Convnet - MNIST\n",
    "\n",
    "Slightly modified from mnist_cnn.py in the Keras examples folder:\n",
    "\n",
    "**https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WEIGHTS_FILEPATH = 'mnist_cnn.hdf5'\n",
    "MODEL_ARCH_FILEPATH = 'mnist_cnn.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "nb_classes = 10\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sequential Model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(32, 3, 3, border_mode='valid', input_shape=input_shape, dim_ordering='tf'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(32, 3, 3, border_mode='valid', dim_ordering='tf'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), border_mode='valid', dim_ordering='tf'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "Epoch 00000: val_acc improved from -inf to 0.98090, saving model to mnist_cnn.hdf5\n",
      "12s - loss: 0.2862 - acc: 0.9141 - val_loss: 0.0603 - val_acc: 0.9809\n",
      "Epoch 2/100\n",
      "Epoch 00001: val_acc improved from 0.98090 to 0.98600, saving model to mnist_cnn.hdf5\n",
      "11s - loss: 0.1023 - acc: 0.9694 - val_loss: 0.0407 - val_acc: 0.9860\n",
      "Epoch 3/100\n",
      "Epoch 00002: val_acc improved from 0.98600 to 0.98940, saving model to mnist_cnn.hdf5\n",
      "11s - loss: 0.0749 - acc: 0.9782 - val_loss: 0.0339 - val_acc: 0.9894\n",
      "Epoch 4/100\n",
      "Epoch 00003: val_acc did not improve\n",
      "11s - loss: 0.0603 - acc: 0.9815 - val_loss: 0.0308 - val_acc: 0.9888\n",
      "Epoch 5/100\n",
      "Epoch 00004: val_acc improved from 0.98940 to 0.99080, saving model to mnist_cnn.hdf5\n",
      "11s - loss: 0.0535 - acc: 0.9836 - val_loss: 0.0297 - val_acc: 0.9908\n",
      "Epoch 6/100\n",
      "Epoch 00005: val_acc improved from 0.99080 to 0.99150, saving model to mnist_cnn.hdf5\n",
      "11s - loss: 0.0481 - acc: 0.9853 - val_loss: 0.0270 - val_acc: 0.9915\n",
      "Epoch 7/100\n",
      "Epoch 00006: val_acc improved from 0.99150 to 0.99200, saving model to mnist_cnn.hdf5\n",
      "11s - loss: 0.0415 - acc: 0.9872 - val_loss: 0.0276 - val_acc: 0.9920\n",
      "Epoch 8/100\n",
      "Epoch 00007: val_acc did not improve\n",
      "11s - loss: 0.0390 - acc: 0.9880 - val_loss: 0.0282 - val_acc: 0.9910\n",
      "Epoch 9/100\n",
      "Epoch 00008: val_acc improved from 0.99200 to 0.99220, saving model to mnist_cnn.hdf5\n",
      "11s - loss: 0.0367 - acc: 0.9884 - val_loss: 0.0278 - val_acc: 0.9922\n",
      "Epoch 10/100\n",
      "Epoch 00009: val_acc did not improve\n",
      "11s - loss: 0.0321 - acc: 0.9895 - val_loss: 0.0286 - val_acc: 0.9915\n",
      "Epoch 11/100\n",
      "Epoch 00010: val_acc did not improve\n",
      "11s - loss: 0.0330 - acc: 0.9895 - val_loss: 0.0269 - val_acc: 0.9917\n",
      "Epoch 12/100\n",
      "Epoch 00011: val_acc improved from 0.99220 to 0.99240, saving model to mnist_cnn.hdf5\n",
      "11s - loss: 0.0278 - acc: 0.9908 - val_loss: 0.0250 - val_acc: 0.9924\n",
      "Epoch 13/100\n",
      "Epoch 00012: val_acc improved from 0.99240 to 0.99260, saving model to mnist_cnn.hdf5\n",
      "11s - loss: 0.0267 - acc: 0.9912 - val_loss: 0.0258 - val_acc: 0.9926\n",
      "Epoch 14/100\n",
      "Epoch 00013: val_acc did not improve\n",
      "11s - loss: 0.0276 - acc: 0.9915 - val_loss: 0.0270 - val_acc: 0.9926\n",
      "Epoch 15/100\n",
      "Epoch 00014: val_acc improved from 0.99260 to 0.99290, saving model to mnist_cnn.hdf5\n",
      "11s - loss: 0.0252 - acc: 0.9916 - val_loss: 0.0254 - val_acc: 0.9929\n",
      "Epoch 16/100\n",
      "Epoch 00015: val_acc did not improve\n",
      "11s - loss: 0.0227 - acc: 0.9925 - val_loss: 0.0252 - val_acc: 0.9924\n",
      "Epoch 17/100\n",
      "Epoch 00016: val_acc did not improve\n",
      "11s - loss: 0.0210 - acc: 0.9928 - val_loss: 0.0297 - val_acc: 0.9922\n",
      "Epoch 18/100\n",
      "Epoch 00017: val_acc did not improve\n",
      "11s - loss: 0.0205 - acc: 0.9930 - val_loss: 0.0264 - val_acc: 0.9922\n",
      "Epoch 19/100\n",
      "Epoch 00018: val_acc did not improve\n",
      "11s - loss: 0.0208 - acc: 0.9931 - val_loss: 0.0282 - val_acc: 0.9924\n",
      "Epoch 20/100\n",
      "Epoch 00019: val_acc did not improve\n",
      "11s - loss: 0.0177 - acc: 0.9942 - val_loss: 0.0269 - val_acc: 0.9924\n",
      "Epoch 21/100\n",
      "Epoch 00020: val_acc improved from 0.99290 to 0.99340, saving model to mnist_cnn.hdf5\n",
      "11s - loss: 0.0197 - acc: 0.9934 - val_loss: 0.0278 - val_acc: 0.9934\n",
      "Epoch 22/100\n",
      "Epoch 00021: val_acc improved from 0.99340 to 0.99370, saving model to mnist_cnn.hdf5\n",
      "11s - loss: 0.0171 - acc: 0.9941 - val_loss: 0.0235 - val_acc: 0.9937\n",
      "Epoch 23/100\n",
      "Epoch 00022: val_acc did not improve\n",
      "11s - loss: 0.0163 - acc: 0.9945 - val_loss: 0.0282 - val_acc: 0.9936\n",
      "Epoch 24/100\n",
      "Epoch 00023: val_acc did not improve\n",
      "11s - loss: 0.0168 - acc: 0.9943 - val_loss: 0.0321 - val_acc: 0.9926\n",
      "Epoch 25/100\n",
      "Epoch 00024: val_acc did not improve\n",
      "11s - loss: 0.0163 - acc: 0.9946 - val_loss: 0.0278 - val_acc: 0.9928\n",
      "Epoch 26/100\n",
      "Epoch 00025: val_acc did not improve\n",
      "11s - loss: 0.0162 - acc: 0.9949 - val_loss: 0.0301 - val_acc: 0.9927\n",
      "Epoch 27/100\n",
      "Epoch 00026: val_acc did not improve\n",
      "11s - loss: 0.0153 - acc: 0.9951 - val_loss: 0.0274 - val_acc: 0.9936\n",
      "Epoch 28/100\n",
      "Epoch 00027: val_acc did not improve\n",
      "Epoch 00027: early stopping\n",
      "11s - loss: 0.0147 - acc: 0.9949 - val_loss: 0.0298 - val_acc: 0.9925\n",
      "Test score: 0.029754588361\n",
      "Test accuracy: 0.9925\n"
     ]
    }
   ],
   "source": [
    "# Model saving callback\n",
    "checkpointer = ModelCheckpoint(filepath=WEIGHTS_FILEPATH, monitor='val_acc', verbose=1, save_best_only=True)\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_acc', verbose=1, patience=5)\n",
    "\n",
    "# Train\n",
    "batch_size = 128\n",
    "nb_epoch = 100\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=2,\n",
    "          callbacks=[checkpointer, early_stopping], \n",
    "          validation_data=(X_test, Y_test))\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(MODEL_ARCH_FILEPATH, 'w') as f:\n",
    "    f.write(model.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
